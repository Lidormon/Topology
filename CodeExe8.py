# -*- coding: utf-8 -*-
"""TM question 8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SwzuL_SFDunTOTImnAWoTBrh7xlaxvE9
"""

import re, numpy as np, pandas as pd, matplotlib.pyplot as plt
import scipy.spatial.distance as ssd
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.decomposition import PCA
import networkx as nx

def load_point_cloud(path:str) -> pd.DataFrame:
    """Decode UTFâ€‘16â€‘LE .csv and return a DataFrame with columns x,y."""
    with open(path, 'rb') as fh:
        txt = fh.read().decode('utf-16-le', errors='ignore')
    nums  = re.findall(r'[-+]?\d*\.\d+|[-+]?\d+', txt)
    pairs = [(float(nums[i]), float(nums[i+1]))
             for i in range(0, len(nums)-1, 2)]
    return pd.DataFrame(pairs, columns=['x', 'y'])

CSV_FILE = '/content/exploratory_data.csv'   # adjust path in Colab as needed
data = load_point_cloud(CSV_FILE)
X    = data.values

print("Preview:")
display(data.head())

plt.scatter(data.x, data.y, s=10)
plt.title("Scatter plot of raw point cloud")
plt.xlabel("x"); plt.ylabel("y")
plt.show()

# Candidate 1: PCAâ€‘PC1
pca    = PCA(n_components=1).fit(X)
pc1    = pca.transform(X).ravel()

# Candidate 2: eccentricity
ecc    = ssd.squareform(ssd.pdist(X)).mean(axis=1)

if pca.explained_variance_ratio_[0] > 0.70:   # heuristic threshold
    f_vals, fname = pc1, "PCAâ€‘PC1"
else:
    f_vals, fname = ecc, "Eccentricity"

print("Chosen filter:", fname)
print("  PCâ€‘1  varâ€‘ratio:", pca.explained_variance_ratio_[0])
print("  Ecc   spread   :", ecc.std())

n_intervals, overlap = 15, 0.50      # tweak here
a, b   = f_vals.min(), f_vals.max()
stride = (b - a) / n_intervals
width  = stride / (1 - overlap)
cover  = [(a + i*stride, a + i*stride + width)
          for i in range(n_intervals)]

print("# intervals:", n_intervals, "  overlap:", overlap)

plt.hist(f_vals, bins=30, color="lightgray", edgecolor="k")
for L, R in cover:
    plt.axvspan(L, R, color="orange", alpha=0.12)
plt.title(f"{fname} histogram with cover"); plt.show()

eps = np.quantile(ssd.pdist(X), 0.05)   # 10â€‘th percentile
print("Îµ (distance threshold) =", eps)

pts_sets = []         # list of pointâ€index sets for each cluster
for cube_id, (L, R) in enumerate(cover):
    idx = np.where((f_vals >= L) & (f_vals <= R))[0]
    if not len(idx):                        # empty cube
        continue
    if len(idx) == 1:                       # singleton
        pts_sets.append({idx[0]})
    else:
        Z    = linkage(X[idx], method='single')
        labs = fcluster(Z, t=eps, criterion='distance')
        pts_sets.extend({*idx[labs == lbl]} for lbl in np.unique(labs))
print("Total clusters (nodes) =", len(pts_sets))

G = nx.Graph()
# add nodes
for i, s in enumerate(pts_sets):
    G.add_node(i, size=len(s), mean_f=f_vals[list(s)].mean())

# add edges (overlapping clusters)
for i in range(len(pts_sets)):
    for j in range(i+1, len(pts_sets)):
        if pts_sets[i] & pts_sets[j]:
            G.add_edge(i, j)

print("nodes:", G.number_of_nodes(), "edges:", G.number_of_edges())

# static plot
pos   = nx.spring_layout(G, seed=42)
sizes = [G.nodes[n]['size']*25 for n in G]
cols  = [G.nodes[n]['mean_f']  for n in G]

plt.figure(figsize=(5,5))
nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=cols, cmap='plasma')
nx.draw_networkx_edges(G, pos, alpha=0.4)
plt.title("Mapper graph"); plt.axis('off'); plt.show()

# Instantiate kmapper
mapper = kmapper.KeplerMapper(verbose=0)

# Fit to data and create a graph
# The 'projected' variable from the previous cells can be used as the projected data
graph = mapper.map(
    f_vals.reshape(-1, 1), # Use the filter values as the projected data
    X, # Use the original data for clustering
    cover=kmapper.Cover(n_cubes=n_intervals, perc_overlap=overlap),
    clusterer=None # Clustering is already done in the previous steps
)

# Visualize the graph
mapper.visualize(graph, path_html="mapper_output.html", title="Mapper Graph")

from google.colab import files

files.download('mapper_output.html')

components = list(nx.connected_components(G))
beta1      = len(nx.cycle_basis(G))
branching  = [n for n,d in G.degree() if d >= 3]

print("Connected components :", len(components))
print("First Betti number Î²â‚:", beta1)
print("Branching nodes (degâ‰¥3):", branching)

if len(components)==1 and beta1==0 and len(branching)>=1:
    inferred_shape = "threeâ€‘arm star (Yâ€‘shape)"
else:
    inferred_shape = "complex / needs inspection"

print("===============================================")
print("Topological conclusion:", inferred_shape)
print("Reasoning:")
print(" â€¢ single component  â‡’ globally connected")
print(" â€¢ Î²â‚ = 0            â‡’ no macroscopic loops")
print(" â€¢ hub + 3 branches  â‡’ Yâ€‘like geometry")

# ğŸ”¹Â CellÂ NÂ â€“Â Summary of answers (i)Â â€“Â (v)
# (run AFTER the earlier cells so the variables already exist)

summary = f"""
(i)Â Filter choice
    â€¢ Chosen filterÂ Â Â Â Â Â Â Â Â Â Â Â Â : {fname}
    â€¢ PCAâ€‘PC1Â varâ€‘ratioÂ Â Â Â Â Â Â Â Â : {pca.explained_variance_ratio_[0]:.3f}
    â€¢ EccentricityÂ spread (std) : {ecc.std():.3f}

(ii)Â Cover parameters
    â€¢ # intervalsÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â : {n_intervals}
    â€¢ OverlapÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â : {overlap:.0%}
    â€¢ Interval widthÂ Â Â Â Â Â Â Â Â Â Â Â : {(cover[0][1]-cover[0][0]):.3f}

(iii)Â Clustering in each cube
    â€¢ AlgorithmÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â : singleâ€‘linkage
    â€¢ ÎµÂ (distance threshold)Â Â Â Â : {eps:.3f}

(iv)Â Mapper graph
    â€¢ Visualisation appears in the previous graph cell
      (static plot or interactive HTML).

(v)Â Topological interpretation
    â€¢ Connected componentsÂ Â Â Â Â Â : {len(components)}
    â€¢ FirstÂ BettiÂ numberÂ Î²â‚Â Â Â Â Â : {beta1}
    â€¢ Branching nodes (degâ‰¥3)Â Â Â : {len(branching)}
    â€¢ Inferred underlying shapeÂ : {inferred_shape}
"""

print(summary)
